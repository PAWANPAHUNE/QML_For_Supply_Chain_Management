{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d1bba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "FEATURE_MAP_REPS_LIST=[1] ANSATZ_REPS_LIST=[1] ENTANGLEMENT_LIST=['linear'] date=06_08_25_1\n",
      "\n",
      "--- Loading and Preprocessing Data ---\n",
      "Total number of data:  14000\n",
      "\n",
      "--- Start K-Fold Loop ---\n",
      "Training data shape: X_train_t: torch.Size([13999, 3]), y_train_t: torch.Size([13999])\n",
      "Testing data shape: X_test_t: torch.Size([1, 3]), y_test_t: torch.Size([1])\n",
      "Assigned feature map parameters: 3\n",
      "Assigned ansatz parameters: 6\n",
      "\n",
      "--- Starting Training 0th---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/1205221547.py:89: DeprecationWarning: The class ``qiskit.primitives.sampler.Sampler`` is deprecated as of qiskit 1.2. It will be removed no earlier than 3 months after the release date. All implementations of the `BaseSamplerV1` interface have been deprecated in favor of their V2 counterparts. The V2 alternative for the `Sampler` class is `StatevectorSampler`.\n",
      "  sampler = Sampler()\n",
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/1205221547.py:91: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.\n",
      "  qnn = SamplerQNN(\n",
      "/Users/pawanpahune/New_Research_Deakin/venv/lib/python3.12/site-packages/qiskit_machine_learning/connectors/torch_connector.py:378: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self._weights.data = torch.tensor(initial_weights, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.6693, Test Loss: 0.8009\n",
      "Epoch 2/100, Train Loss: 0.6626, Test Loss: 0.7899\n",
      "Epoch 3/100, Train Loss: 0.6625, Test Loss: 0.7847\n",
      "Epoch 4/100, Train Loss: 0.6624, Test Loss: 0.7989\n",
      "Epoch 5/100, Train Loss: 0.6625, Test Loss: 0.8171\n",
      "Epoch 6/100, Train Loss: 0.6624, Test Loss: 0.8164\n",
      "Epoch 7/100, Train Loss: 0.6625, Test Loss: 0.7909\n",
      "Epoch 8/100, Train Loss: 0.6625, Test Loss: 0.7834\n",
      "Epoch 9/100, Train Loss: 0.6624, Test Loss: 0.8061\n",
      "Epoch 10/100, Train Loss: 0.6624, Test Loss: 0.8059\n",
      "Epoch 11/100, Train Loss: 0.6624, Test Loss: 0.8041\n",
      "Epoch 12/100, Train Loss: 0.6625, Test Loss: 0.7885\n",
      "Epoch 13/100, Train Loss: 0.6625, Test Loss: 0.8006\n",
      "Epoch 14/100, Train Loss: 0.6625, Test Loss: 0.7978\n",
      "Epoch 15/100, Train Loss: 0.6625, Test Loss: 0.7911\n",
      "Epoch 16/100, Train Loss: 0.6624, Test Loss: 0.8093\n",
      "Epoch 17/100, Train Loss: 0.6626, Test Loss: 0.8029\n",
      "Epoch 18/100, Train Loss: 0.6625, Test Loss: 0.7752\n",
      "Epoch 19/100, Train Loss: 0.6625, Test Loss: 0.8052\n",
      "Epoch 20/100, Train Loss: 0.6623, Test Loss: 0.7850\n",
      "Epoch 21/100, Train Loss: 0.6624, Test Loss: 0.8057\n",
      "Epoch 22/100, Train Loss: 0.6625, Test Loss: 0.7894\n",
      "Epoch 23/100, Train Loss: 0.6625, Test Loss: 0.8227\n",
      "Epoch 24/100, Train Loss: 0.6625, Test Loss: 0.7988\n",
      "Epoch 25/100, Train Loss: 0.6624, Test Loss: 0.8167\n",
      "Epoch 26/100, Train Loss: 0.6626, Test Loss: 0.7859\n",
      "Epoch 27/100, Train Loss: 0.6624, Test Loss: 0.8116\n",
      "Epoch 28/100, Train Loss: 0.6626, Test Loss: 0.8039\n",
      "Epoch 29/100, Train Loss: 0.6625, Test Loss: 0.7832\n",
      "Epoch 30/100, Train Loss: 0.6625, Test Loss: 0.8150\n",
      "Epoch 31/100, Train Loss: 0.6625, Test Loss: 0.7675\n",
      "Epoch 32/100, Train Loss: 0.6625, Test Loss: 0.7923\n",
      "Epoch 33/100, Train Loss: 0.6624, Test Loss: 0.7927\n",
      "Epoch 34/100, Train Loss: 0.6625, Test Loss: 0.7961\n",
      "Epoch 35/100, Train Loss: 0.6626, Test Loss: 0.8086\n",
      "Epoch 36/100, Train Loss: 0.6625, Test Loss: 0.7853\n",
      "Epoch 37/100, Train Loss: 0.6624, Test Loss: 0.7887\n",
      "Epoch 38/100, Train Loss: 0.6627, Test Loss: 0.7944\n",
      "Epoch 39/100, Train Loss: 0.6625, Test Loss: 0.7761\n",
      "Epoch 40/100, Train Loss: 0.6624, Test Loss: 0.8201\n",
      "Epoch 41/100, Train Loss: 0.6625, Test Loss: 0.7838\n",
      "Epoch 42/100, Train Loss: 0.6625, Test Loss: 0.7929\n",
      "Epoch 43/100, Train Loss: 0.6624, Test Loss: 0.7732\n",
      "Epoch 44/100, Train Loss: 0.6625, Test Loss: 0.7748\n",
      "Epoch 45/100, Train Loss: 0.6626, Test Loss: 0.8133\n",
      "Epoch 46/100, Train Loss: 0.6625, Test Loss: 0.7928\n",
      "Epoch 47/100, Train Loss: 0.6625, Test Loss: 0.7945\n",
      "Epoch 48/100, Train Loss: 0.6624, Test Loss: 0.7740\n",
      "Epoch 49/100, Train Loss: 0.6626, Test Loss: 0.7902\n",
      "Epoch 50/100, Train Loss: 0.6624, Test Loss: 0.7882\n",
      "Epoch 51/100, Train Loss: 0.6623, Test Loss: 0.7884\n",
      "Epoch 52/100, Train Loss: 0.6626, Test Loss: 0.7878\n",
      "Epoch 53/100, Train Loss: 0.6625, Test Loss: 0.8037\n",
      "Epoch 54/100, Train Loss: 0.6624, Test Loss: 0.7899\n",
      "Epoch 55/100, Train Loss: 0.6625, Test Loss: 0.8066\n",
      "Epoch 56/100, Train Loss: 0.6624, Test Loss: 0.7834\n",
      "Epoch 57/100, Train Loss: 0.6626, Test Loss: 0.7642\n",
      "Epoch 58/100, Train Loss: 0.6626, Test Loss: 0.7901\n",
      "Epoch 59/100, Train Loss: 0.6625, Test Loss: 0.8100\n",
      "Epoch 60/100, Train Loss: 0.6624, Test Loss: 0.7784\n",
      "Epoch 61/100, Train Loss: 0.6626, Test Loss: 0.7862\n",
      "Epoch 62/100, Train Loss: 0.6625, Test Loss: 0.7733\n",
      "Epoch 63/100, Train Loss: 0.6625, Test Loss: 0.8107\n",
      "Epoch 64/100, Train Loss: 0.6625, Test Loss: 0.7943\n",
      "Epoch 65/100, Train Loss: 0.6625, Test Loss: 0.7943\n",
      "Epoch 66/100, Train Loss: 0.6624, Test Loss: 0.8030\n",
      "Epoch 67/100, Train Loss: 0.6624, Test Loss: 0.8141\n",
      "Epoch 68/100, Train Loss: 0.6625, Test Loss: 0.7756\n",
      "Epoch 69/100, Train Loss: 0.6625, Test Loss: 0.8116\n",
      "Epoch 70/100, Train Loss: 0.6625, Test Loss: 0.7875\n",
      "Epoch 71/100, Train Loss: 0.6624, Test Loss: 0.7779\n",
      "Epoch 72/100, Train Loss: 0.6624, Test Loss: 0.7782\n",
      "Epoch 73/100, Train Loss: 0.6625, Test Loss: 0.7662\n",
      "Epoch 74/100, Train Loss: 0.6626, Test Loss: 0.7785\n",
      "Epoch 75/100, Train Loss: 0.6625, Test Loss: 0.7879\n",
      "Epoch 76/100, Train Loss: 0.6624, Test Loss: 0.7909\n",
      "Epoch 77/100, Train Loss: 0.6625, Test Loss: 0.8185\n",
      "Epoch 78/100, Train Loss: 0.6626, Test Loss: 0.8128\n",
      "Epoch 79/100, Train Loss: 0.6626, Test Loss: 0.8020\n",
      "Epoch 80/100, Train Loss: 0.6626, Test Loss: 0.7807\n",
      "Epoch 81/100, Train Loss: 0.6625, Test Loss: 0.7917\n",
      "Epoch 82/100, Train Loss: 0.6625, Test Loss: 0.8105\n",
      "Epoch 83/100, Train Loss: 0.6624, Test Loss: 0.8001\n",
      "Epoch 84/100, Train Loss: 0.6624, Test Loss: 0.8000\n",
      "Epoch 85/100, Train Loss: 0.6626, Test Loss: 0.7740\n",
      "Epoch 86/100, Train Loss: 0.6624, Test Loss: 0.7659\n",
      "Epoch 87/100, Train Loss: 0.6626, Test Loss: 0.8045\n",
      "Epoch 88/100, Train Loss: 0.6624, Test Loss: 0.8064\n",
      "Epoch 89/100, Train Loss: 0.6624, Test Loss: 0.8034\n",
      "Epoch 90/100, Train Loss: 0.6624, Test Loss: 0.7609\n",
      "Epoch 91/100, Train Loss: 0.6624, Test Loss: 0.8006\n",
      "Epoch 92/100, Train Loss: 0.6626, Test Loss: 0.7901\n",
      "Epoch 93/100, Train Loss: 0.6623, Test Loss: 0.8011\n",
      "Epoch 94/100, Train Loss: 0.6623, Test Loss: 0.7927\n",
      "Epoch 95/100, Train Loss: 0.6625, Test Loss: 0.8188\n",
      "Epoch 96/100, Train Loss: 0.6625, Test Loss: 0.7971\n",
      "Epoch 97/100, Train Loss: 0.6624, Test Loss: 0.8154\n",
      "Epoch 98/100, Train Loss: 0.6624, Test Loss: 0.8010\n",
      "Epoch 99/100, Train Loss: 0.6626, Test Loss: 0.7960\n",
      "Epoch 100/100, Train Loss: 0.6623, Test Loss: 0.7873\n",
      "--- Training Finished ---\n",
      "\n",
      "--- Metrics for entanglement: linear, feature_map_reps: 1, ansatz_reps: 1 ---\n",
      "Train Accuracy: 0.5788\n",
      "Test Accuracy: 0.0000\n",
      "Train F1 Score: 0.6342\n",
      "Test F1 Score: 0.0000\n",
      "Train Recall: 0.7301\n",
      "Test Recall: 0.0000\n",
      "Train Precision: 0.5605\n",
      "Test Precision: 0.0000\n",
      "Train Confusion Matrix:\n",
      "[[2992 4007]\n",
      " [1889 5111]]\n",
      "Test Confusion Matrix:\n",
      "[[0 1]\n",
      " [0 0]]\n",
      "0\n",
      "0\n",
      "\n",
      "--- Done for entanglement: linear, feature_map_reps: 1, ansatz_reps: 1 ---\n",
      "Training data shape: X_train_t: torch.Size([13999, 3]), y_train_t: torch.Size([13999])\n",
      "Testing data shape: X_test_t: torch.Size([1, 3]), y_test_t: torch.Size([1])\n",
      "Assigned feature map parameters: 3\n",
      "Assigned ansatz parameters: 6\n",
      "\n",
      "--- Starting Training 1th---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/1205221547.py:89: DeprecationWarning: The class ``qiskit.primitives.sampler.Sampler`` is deprecated as of qiskit 1.2. It will be removed no earlier than 3 months after the release date. All implementations of the `BaseSamplerV1` interface have been deprecated in favor of their V2 counterparts. The V2 alternative for the `Sampler` class is `StatevectorSampler`.\n",
      "  sampler = Sampler()\n",
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/1205221547.py:91: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.\n",
      "  qnn = SamplerQNN(\n",
      "/Users/pawanpahune/New_Research_Deakin/venv/lib/python3.12/site-packages/qiskit_machine_learning/connectors/torch_connector.py:378: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self._weights.data = torch.tensor(initial_weights, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.6416, Test Loss: 0.5388\n",
      "Epoch 2/100, Train Loss: 0.6186, Test Loss: 0.5356\n",
      "Epoch 3/100, Train Loss: 0.6186, Test Loss: 0.5126\n",
      "Epoch 4/100, Train Loss: 0.6186, Test Loss: 0.5460\n",
      "Epoch 5/100, Train Loss: 0.6186, Test Loss: 0.5193\n",
      "Epoch 6/100, Train Loss: 0.6186, Test Loss: 0.5516\n",
      "Epoch 7/100, Train Loss: 0.6186, Test Loss: 0.5157\n",
      "Epoch 8/100, Train Loss: 0.6188, Test Loss: 0.5311\n",
      "Epoch 9/100, Train Loss: 0.6186, Test Loss: 0.5437\n",
      "Epoch 10/100, Train Loss: 0.6187, Test Loss: 0.5223\n",
      "Epoch 11/100, Train Loss: 0.6187, Test Loss: 0.5307\n",
      "Epoch 12/100, Train Loss: 0.6187, Test Loss: 0.5426\n",
      "Epoch 13/100, Train Loss: 0.6185, Test Loss: 0.5092\n",
      "Epoch 14/100, Train Loss: 0.6186, Test Loss: 0.5209\n",
      "Epoch 15/100, Train Loss: 0.6186, Test Loss: 0.5181\n",
      "Epoch 16/100, Train Loss: 0.6186, Test Loss: 0.5361\n",
      "Epoch 17/100, Train Loss: 0.6187, Test Loss: 0.5216\n",
      "Epoch 18/100, Train Loss: 0.6186, Test Loss: 0.5518\n",
      "Epoch 19/100, Train Loss: 0.6187, Test Loss: 0.5167\n",
      "Epoch 20/100, Train Loss: 0.6185, Test Loss: 0.5393\n",
      "Epoch 21/100, Train Loss: 0.6187, Test Loss: 0.5281\n",
      "Epoch 22/100, Train Loss: 0.6187, Test Loss: 0.5333\n",
      "Epoch 23/100, Train Loss: 0.6186, Test Loss: 0.5244\n",
      "Epoch 24/100, Train Loss: 0.6187, Test Loss: 0.5416\n",
      "Epoch 25/100, Train Loss: 0.6187, Test Loss: 0.5282\n",
      "Epoch 26/100, Train Loss: 0.6186, Test Loss: 0.5154\n",
      "Epoch 27/100, Train Loss: 0.6188, Test Loss: 0.5430\n",
      "Epoch 28/100, Train Loss: 0.6186, Test Loss: 0.5211\n",
      "Epoch 29/100, Train Loss: 0.6185, Test Loss: 0.5277\n",
      "Epoch 30/100, Train Loss: 0.6186, Test Loss: 0.5238\n",
      "Epoch 31/100, Train Loss: 0.6187, Test Loss: 0.5303\n",
      "Epoch 32/100, Train Loss: 0.6187, Test Loss: 0.5150\n",
      "Epoch 33/100, Train Loss: 0.6187, Test Loss: 0.5243\n",
      "Epoch 34/100, Train Loss: 0.6186, Test Loss: 0.5319\n",
      "Epoch 35/100, Train Loss: 0.6186, Test Loss: 0.5480\n",
      "Epoch 36/100, Train Loss: 0.6187, Test Loss: 0.5336\n",
      "Epoch 37/100, Train Loss: 0.6185, Test Loss: 0.5536\n",
      "Epoch 38/100, Train Loss: 0.6187, Test Loss: 0.5260\n",
      "Epoch 39/100, Train Loss: 0.6187, Test Loss: 0.5302\n",
      "Epoch 40/100, Train Loss: 0.6186, Test Loss: 0.5279\n",
      "Epoch 41/100, Train Loss: 0.6186, Test Loss: 0.5210\n",
      "Epoch 42/100, Train Loss: 0.6186, Test Loss: 0.5235\n",
      "Epoch 43/100, Train Loss: 0.6187, Test Loss: 0.5457\n",
      "Epoch 44/100, Train Loss: 0.6186, Test Loss: 0.5348\n",
      "Epoch 45/100, Train Loss: 0.6186, Test Loss: 0.5393\n",
      "Epoch 46/100, Train Loss: 0.6186, Test Loss: 0.5290\n",
      "Epoch 47/100, Train Loss: 0.6187, Test Loss: 0.5442\n",
      "Epoch 48/100, Train Loss: 0.6186, Test Loss: 0.5104\n",
      "Epoch 49/100, Train Loss: 0.6187, Test Loss: 0.5437\n",
      "Epoch 50/100, Train Loss: 0.6186, Test Loss: 0.5323\n",
      "Epoch 51/100, Train Loss: 0.6187, Test Loss: 0.5419\n",
      "Epoch 52/100, Train Loss: 0.6186, Test Loss: 0.5327\n",
      "Epoch 53/100, Train Loss: 0.6187, Test Loss: 0.5231\n",
      "Epoch 54/100, Train Loss: 0.6187, Test Loss: 0.5433\n",
      "Epoch 55/100, Train Loss: 0.6186, Test Loss: 0.5303\n",
      "Epoch 56/100, Train Loss: 0.6187, Test Loss: 0.5323\n",
      "Epoch 57/100, Train Loss: 0.6187, Test Loss: 0.5492\n",
      "Epoch 58/100, Train Loss: 0.6186, Test Loss: 0.5276\n",
      "Epoch 59/100, Train Loss: 0.6188, Test Loss: 0.5359\n",
      "Epoch 60/100, Train Loss: 0.6187, Test Loss: 0.5216\n",
      "Epoch 61/100, Train Loss: 0.6187, Test Loss: 0.5353\n",
      "Epoch 62/100, Train Loss: 0.6187, Test Loss: 0.5190\n",
      "Epoch 63/100, Train Loss: 0.6187, Test Loss: 0.5198\n",
      "Epoch 64/100, Train Loss: 0.6186, Test Loss: 0.5090\n",
      "Epoch 65/100, Train Loss: 0.6187, Test Loss: 0.5243\n",
      "Epoch 66/100, Train Loss: 0.6187, Test Loss: 0.5358\n",
      "Epoch 67/100, Train Loss: 0.6186, Test Loss: 0.5458\n",
      "Epoch 68/100, Train Loss: 0.6187, Test Loss: 0.5280\n",
      "Epoch 69/100, Train Loss: 0.6187, Test Loss: 0.5348\n",
      "Epoch 70/100, Train Loss: 0.6186, Test Loss: 0.5379\n",
      "Epoch 71/100, Train Loss: 0.6186, Test Loss: 0.5366\n",
      "Epoch 72/100, Train Loss: 0.6187, Test Loss: 0.5213\n",
      "Epoch 73/100, Train Loss: 0.6187, Test Loss: 0.5543\n",
      "Epoch 74/100, Train Loss: 0.6187, Test Loss: 0.5240\n",
      "Epoch 75/100, Train Loss: 0.6186, Test Loss: 0.5295\n",
      "Epoch 76/100, Train Loss: 0.6186, Test Loss: 0.5315\n",
      "Epoch 77/100, Train Loss: 0.6186, Test Loss: 0.5419\n",
      "Epoch 78/100, Train Loss: 0.6187, Test Loss: 0.5096\n",
      "Epoch 79/100, Train Loss: 0.6186, Test Loss: 0.5396\n",
      "Epoch 80/100, Train Loss: 0.6187, Test Loss: 0.5324\n",
      "Epoch 81/100, Train Loss: 0.6187, Test Loss: 0.5243\n",
      "Epoch 82/100, Train Loss: 0.6187, Test Loss: 0.5379\n",
      "Epoch 83/100, Train Loss: 0.6187, Test Loss: 0.5210\n",
      "Epoch 84/100, Train Loss: 0.6187, Test Loss: 0.5342\n",
      "Epoch 85/100, Train Loss: 0.6187, Test Loss: 0.5265\n",
      "Epoch 86/100, Train Loss: 0.6187, Test Loss: 0.5421\n",
      "Epoch 87/100, Train Loss: 0.6186, Test Loss: 0.5276\n",
      "Epoch 88/100, Train Loss: 0.6187, Test Loss: 0.5399\n",
      "Epoch 89/100, Train Loss: 0.6187, Test Loss: 0.5281\n",
      "Epoch 90/100, Train Loss: 0.6187, Test Loss: 0.5440\n",
      "Epoch 91/100, Train Loss: 0.6187, Test Loss: 0.5321\n",
      "Epoch 92/100, Train Loss: 0.6187, Test Loss: 0.5277\n",
      "Epoch 93/100, Train Loss: 0.6186, Test Loss: 0.5352\n",
      "Epoch 94/100, Train Loss: 0.6186, Test Loss: 0.5276\n",
      "Epoch 95/100, Train Loss: 0.6187, Test Loss: 0.5256\n",
      "Epoch 96/100, Train Loss: 0.6187, Test Loss: 0.5201\n",
      "Epoch 97/100, Train Loss: 0.6186, Test Loss: 0.5083\n",
      "Epoch 98/100, Train Loss: 0.6187, Test Loss: 0.5275\n",
      "Epoch 99/100, Train Loss: 0.6186, Test Loss: 0.5513\n",
      "Epoch 100/100, Train Loss: 0.6186, Test Loss: 0.5420\n",
      "--- Training Finished ---\n",
      "\n",
      "--- Metrics for entanglement: linear, feature_map_reps: 1, ansatz_reps: 1 ---\n",
      "Train Accuracy: 0.7278\n",
      "Test Accuracy: 1.0000\n",
      "Train F1 Score: 0.7427\n",
      "Test F1 Score: 1.0000\n",
      "Train Recall: 0.7858\n",
      "Test Recall: 1.0000\n",
      "Train Precision: 0.7040\n",
      "Test Precision: 1.0000\n",
      "Train Confusion Matrix:\n",
      "[[4688 2312]\n",
      " [1499 5500]]\n",
      "Test Confusion Matrix:\n",
      "[[0 0]\n",
      " [0 1]]\n",
      "0\n",
      "0\n",
      "\n",
      "--- Done for entanglement: linear, feature_map_reps: 1, ansatz_reps: 1 ---\n",
      "Training data shape: X_train_t: torch.Size([13999, 3]), y_train_t: torch.Size([13999])\n",
      "Testing data shape: X_test_t: torch.Size([1, 3]), y_test_t: torch.Size([1])\n",
      "Assigned feature map parameters: 3\n",
      "Assigned ansatz parameters: 6\n",
      "\n",
      "--- Starting Training 2th---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/1205221547.py:89: DeprecationWarning: The class ``qiskit.primitives.sampler.Sampler`` is deprecated as of qiskit 1.2. It will be removed no earlier than 3 months after the release date. All implementations of the `BaseSamplerV1` interface have been deprecated in favor of their V2 counterparts. The V2 alternative for the `Sampler` class is `StatevectorSampler`.\n",
      "  sampler = Sampler()\n",
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/1205221547.py:91: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.\n",
      "  qnn = SamplerQNN(\n",
      "/Users/pawanpahune/New_Research_Deakin/venv/lib/python3.12/site-packages/qiskit_machine_learning/connectors/torch_connector.py:378: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self._weights.data = torch.tensor(initial_weights, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.6408, Test Loss: 0.5438\n",
      "Epoch 2/100, Train Loss: 0.6187, Test Loss: 0.5407\n",
      "Epoch 3/100, Train Loss: 0.6187, Test Loss: 0.5329\n",
      "Epoch 4/100, Train Loss: 0.6187, Test Loss: 0.5509\n",
      "Epoch 5/100, Train Loss: 0.6185, Test Loss: 0.5737\n",
      "Epoch 6/100, Train Loss: 0.6185, Test Loss: 0.5749\n",
      "Epoch 7/100, Train Loss: 0.6186, Test Loss: 0.5680\n",
      "Epoch 8/100, Train Loss: 0.6187, Test Loss: 0.5544\n",
      "Epoch 9/100, Train Loss: 0.6187, Test Loss: 0.5697\n",
      "Epoch 10/100, Train Loss: 0.6186, Test Loss: 0.5629\n",
      "Epoch 11/100, Train Loss: 0.6186, Test Loss: 0.5746\n",
      "Epoch 12/100, Train Loss: 0.6187, Test Loss: 0.5607\n",
      "Epoch 13/100, Train Loss: 0.6186, Test Loss: 0.5426\n",
      "Epoch 14/100, Train Loss: 0.6186, Test Loss: 0.5625\n",
      "Epoch 15/100, Train Loss: 0.6186, Test Loss: 0.5401\n",
      "Epoch 16/100, Train Loss: 0.6186, Test Loss: 0.5461\n",
      "Epoch 17/100, Train Loss: 0.6187, Test Loss: 0.5370\n",
      "Epoch 18/100, Train Loss: 0.6186, Test Loss: 0.5677\n",
      "Epoch 19/100, Train Loss: 0.6186, Test Loss: 0.5512\n",
      "Epoch 20/100, Train Loss: 0.6187, Test Loss: 0.5630\n",
      "Epoch 21/100, Train Loss: 0.6186, Test Loss: 0.5583\n",
      "Epoch 22/100, Train Loss: 0.6185, Test Loss: 0.5441\n",
      "Epoch 23/100, Train Loss: 0.6187, Test Loss: 0.5489\n",
      "Epoch 24/100, Train Loss: 0.6187, Test Loss: 0.5539\n",
      "Epoch 25/100, Train Loss: 0.6186, Test Loss: 0.5679\n",
      "Epoch 26/100, Train Loss: 0.6186, Test Loss: 0.5625\n",
      "Epoch 27/100, Train Loss: 0.6185, Test Loss: 0.5809\n",
      "Epoch 28/100, Train Loss: 0.6185, Test Loss: 0.6038\n",
      "Epoch 29/100, Train Loss: 0.6187, Test Loss: 0.5503\n",
      "Epoch 30/100, Train Loss: 0.6187, Test Loss: 0.5542\n",
      "Epoch 31/100, Train Loss: 0.6187, Test Loss: 0.5535\n",
      "Epoch 32/100, Train Loss: 0.6186, Test Loss: 0.5564\n",
      "Epoch 33/100, Train Loss: 0.6186, Test Loss: 0.5776\n",
      "Epoch 34/100, Train Loss: 0.6187, Test Loss: 0.5426\n",
      "Epoch 35/100, Train Loss: 0.6186, Test Loss: 0.5429\n",
      "Epoch 36/100, Train Loss: 0.6187, Test Loss: 0.5307\n",
      "Epoch 37/100, Train Loss: 0.6185, Test Loss: 0.5853\n",
      "Epoch 38/100, Train Loss: 0.6187, Test Loss: 0.5502\n",
      "Epoch 39/100, Train Loss: 0.6187, Test Loss: 0.5524\n",
      "Epoch 40/100, Train Loss: 0.6186, Test Loss: 0.5570\n",
      "Epoch 41/100, Train Loss: 0.6186, Test Loss: 0.5382\n",
      "Epoch 42/100, Train Loss: 0.6187, Test Loss: 0.5710\n",
      "Epoch 43/100, Train Loss: 0.6186, Test Loss: 0.5623\n",
      "Epoch 44/100, Train Loss: 0.6187, Test Loss: 0.5346\n",
      "Epoch 45/100, Train Loss: 0.6186, Test Loss: 0.5763\n",
      "Epoch 46/100, Train Loss: 0.6186, Test Loss: 0.5662\n",
      "Epoch 47/100, Train Loss: 0.6187, Test Loss: 0.5627\n",
      "Epoch 48/100, Train Loss: 0.6187, Test Loss: 0.5639\n",
      "Epoch 49/100, Train Loss: 0.6186, Test Loss: 0.5800\n",
      "Epoch 50/100, Train Loss: 0.6187, Test Loss: 0.5580\n",
      "Epoch 51/100, Train Loss: 0.6186, Test Loss: 0.5471\n",
      "Epoch 52/100, Train Loss: 0.6186, Test Loss: 0.5576\n",
      "Epoch 53/100, Train Loss: 0.6187, Test Loss: 0.5629\n",
      "Epoch 54/100, Train Loss: 0.6187, Test Loss: 0.5404\n",
      "Epoch 55/100, Train Loss: 0.6187, Test Loss: 0.5526\n",
      "Epoch 56/100, Train Loss: 0.6186, Test Loss: 0.5648\n",
      "Epoch 57/100, Train Loss: 0.6187, Test Loss: 0.5598\n",
      "Epoch 58/100, Train Loss: 0.6186, Test Loss: 0.5627\n",
      "Epoch 59/100, Train Loss: 0.6187, Test Loss: 0.5391\n",
      "Epoch 60/100, Train Loss: 0.6186, Test Loss: 0.5243\n",
      "Epoch 61/100, Train Loss: 0.6188, Test Loss: 0.5409\n",
      "Epoch 62/100, Train Loss: 0.6186, Test Loss: 0.5641\n",
      "Epoch 63/100, Train Loss: 0.6186, Test Loss: 0.5587\n",
      "Epoch 64/100, Train Loss: 0.6185, Test Loss: 0.5696\n",
      "Epoch 65/100, Train Loss: 0.6187, Test Loss: 0.5413\n",
      "Epoch 66/100, Train Loss: 0.6186, Test Loss: 0.5454\n",
      "Epoch 67/100, Train Loss: 0.6187, Test Loss: 0.5626\n",
      "Epoch 68/100, Train Loss: 0.6185, Test Loss: 0.5410\n",
      "Epoch 69/100, Train Loss: 0.6186, Test Loss: 0.5529\n",
      "Epoch 70/100, Train Loss: 0.6187, Test Loss: 0.5422\n",
      "Epoch 71/100, Train Loss: 0.6186, Test Loss: 0.5604\n",
      "Epoch 72/100, Train Loss: 0.6187, Test Loss: 0.5427\n",
      "Epoch 73/100, Train Loss: 0.6186, Test Loss: 0.5794\n",
      "Epoch 74/100, Train Loss: 0.6186, Test Loss: 0.5628\n",
      "Epoch 75/100, Train Loss: 0.6186, Test Loss: 0.5632\n",
      "Epoch 76/100, Train Loss: 0.6187, Test Loss: 0.5327\n",
      "Epoch 77/100, Train Loss: 0.6186, Test Loss: 0.5381\n",
      "Epoch 78/100, Train Loss: 0.6187, Test Loss: 0.5763\n",
      "Epoch 79/100, Train Loss: 0.6187, Test Loss: 0.5652\n",
      "Epoch 80/100, Train Loss: 0.6187, Test Loss: 0.5336\n",
      "Epoch 81/100, Train Loss: 0.6187, Test Loss: 0.5622\n",
      "Epoch 82/100, Train Loss: 0.6185, Test Loss: 0.5659\n",
      "Epoch 83/100, Train Loss: 0.6187, Test Loss: 0.5497\n",
      "Epoch 84/100, Train Loss: 0.6187, Test Loss: 0.5506\n",
      "Epoch 85/100, Train Loss: 0.6186, Test Loss: 0.5508\n",
      "Epoch 86/100, Train Loss: 0.6186, Test Loss: 0.5715\n",
      "Epoch 87/100, Train Loss: 0.6186, Test Loss: 0.5545\n",
      "Epoch 88/100, Train Loss: 0.6186, Test Loss: 0.5415\n",
      "Epoch 89/100, Train Loss: 0.6187, Test Loss: 0.5432\n",
      "Epoch 90/100, Train Loss: 0.6186, Test Loss: 0.5730\n",
      "Epoch 91/100, Train Loss: 0.6187, Test Loss: 0.5509\n",
      "Epoch 92/100, Train Loss: 0.6186, Test Loss: 0.5478\n",
      "Epoch 93/100, Train Loss: 0.6186, Test Loss: 0.5865\n",
      "Epoch 94/100, Train Loss: 0.6186, Test Loss: 0.5390\n",
      "Epoch 95/100, Train Loss: 0.6187, Test Loss: 0.5738\n",
      "Epoch 96/100, Train Loss: 0.6187, Test Loss: 0.5713\n",
      "Epoch 97/100, Train Loss: 0.6186, Test Loss: 0.5047\n",
      "Epoch 98/100, Train Loss: 0.6185, Test Loss: 0.5866\n",
      "Epoch 99/100, Train Loss: 0.6187, Test Loss: 0.5753\n",
      "Epoch 100/100, Train Loss: 0.6186, Test Loss: 0.5540\n",
      "--- Training Finished ---\n",
      "\n",
      "--- Metrics for entanglement: linear, feature_map_reps: 1, ansatz_reps: 1 ---\n",
      "Train Accuracy: 0.7277\n",
      "Test Accuracy: 1.0000\n",
      "Train F1 Score: 0.7421\n",
      "Test F1 Score: 1.0000\n",
      "Train Recall: 0.7835\n",
      "Test Recall: 1.0000\n",
      "Train Precision: 0.7048\n",
      "Test Precision: 1.0000\n",
      "Train Confusion Matrix:\n",
      "[[4703 2297]\n",
      " [1515 5484]]\n",
      "Test Confusion Matrix:\n",
      "[[0 0]\n",
      " [0 1]]\n",
      "0\n",
      "0\n",
      "\n",
      "--- Done for entanglement: linear, feature_map_reps: 1, ansatz_reps: 1 ---\n",
      "Training data shape: X_train_t: torch.Size([13999, 3]), y_train_t: torch.Size([13999])\n",
      "Testing data shape: X_test_t: torch.Size([1, 3]), y_test_t: torch.Size([1])\n",
      "Assigned feature map parameters: 3\n",
      "Assigned ansatz parameters: 6\n",
      "\n",
      "--- Starting Training 3th---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/1205221547.py:89: DeprecationWarning: The class ``qiskit.primitives.sampler.Sampler`` is deprecated as of qiskit 1.2. It will be removed no earlier than 3 months after the release date. All implementations of the `BaseSamplerV1` interface have been deprecated in favor of their V2 counterparts. The V2 alternative for the `Sampler` class is `StatevectorSampler`.\n",
      "  sampler = Sampler()\n",
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/1205221547.py:91: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.\n",
      "  qnn = SamplerQNN(\n",
      "/Users/pawanpahune/New_Research_Deakin/venv/lib/python3.12/site-packages/qiskit_machine_learning/connectors/torch_connector.py:378: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self._weights.data = torch.tensor(initial_weights, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.6696, Test Loss: 0.4673\n",
      "Epoch 2/100, Train Loss: 0.6625, Test Loss: 0.4544\n",
      "Epoch 3/100, Train Loss: 0.6625, Test Loss: 0.4582\n",
      "Epoch 4/100, Train Loss: 0.6625, Test Loss: 0.4504\n",
      "Epoch 5/100, Train Loss: 0.6624, Test Loss: 0.4539\n",
      "Epoch 6/100, Train Loss: 0.6626, Test Loss: 0.4459\n",
      "Epoch 7/100, Train Loss: 0.6626, Test Loss: 0.4600\n",
      "Epoch 8/100, Train Loss: 0.6625, Test Loss: 0.4530\n",
      "Epoch 9/100, Train Loss: 0.6626, Test Loss: 0.4619\n",
      "Epoch 10/100, Train Loss: 0.6626, Test Loss: 0.4464\n",
      "Epoch 11/100, Train Loss: 0.6625, Test Loss: 0.4781\n",
      "Epoch 12/100, Train Loss: 0.6625, Test Loss: 0.4499\n",
      "Epoch 13/100, Train Loss: 0.6625, Test Loss: 0.4387\n",
      "Epoch 14/100, Train Loss: 0.6625, Test Loss: 0.4566\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 261\u001b[39m\n\u001b[32m    259\u001b[39m outputs = model(batch_X)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m    260\u001b[39m loss = LOSS(outputs, batch_y)  \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backward pass (compute gradients)\u001b[39;00m\n\u001b[32m    262\u001b[39m optimizer.step()  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[32m    263\u001b[39m running_loss += loss.item() * batch_X.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/New_Research_Deakin/venv/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/New_Research_Deakin/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/New_Research_Deakin/venv/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/New_Research_Deakin/venv/lib/python3.12/site-packages/torch/autograd/function.py:307\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    302\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    303\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    304\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    305\u001b[39m     )\n\u001b[32m    306\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/New_Research_Deakin/venv/lib/python3.12/site-packages/qiskit_machine_learning/connectors/torch_connector.py:229\u001b[39m, in \u001b[36m_TorchNNFunction.backward\u001b[39m\u001b[34m(ctx, grad_output)\u001b[39m\n\u001b[32m    226\u001b[39m     grad_output = grad_output.view(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# evaluate QNN gradient\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m input_grad, weights_grad = \u001b[43mneural_network\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ctx.sparse:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/New_Research_Deakin/venv/lib/python3.12/site-packages/qiskit_machine_learning/neural_networks/neural_network.py:257\u001b[39m, in \u001b[36mNeuralNetwork.backward\u001b[39m\u001b[34m(self, input_data, weights)\u001b[39m\n\u001b[32m    255\u001b[39m input_, shape = \u001b[38;5;28mself\u001b[39m._validate_input(input_data)\n\u001b[32m    256\u001b[39m weights_ = \u001b[38;5;28mself\u001b[39m._validate_weights(weights)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m input_grad, weight_grad = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m input_grad_reshaped, weight_grad_reshaped = \u001b[38;5;28mself\u001b[39m._validate_backward_output(\n\u001b[32m    260\u001b[39m     input_grad, weight_grad, shape\n\u001b[32m    261\u001b[39m )\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m input_grad_reshaped, weight_grad_reshaped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/New_Research_Deakin/venv/lib/python3.12/site-packages/qiskit_machine_learning/neural_networks/sampler_qnn.py:527\u001b[39m, in \u001b[36mSamplerQNN._backward\u001b[39m\u001b[34m(self, input_data, weights)\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    526\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m         results = \u001b[43mjob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    528\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    529\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m QiskitMachineLearningError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSampler job failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/New_Research_Deakin/venv/lib/python3.12/site-packages/qiskit/primitives/primitive_job.py:51\u001b[39m, in \u001b[36mPrimitiveJob.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ResultT:\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_submitted()\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_future\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/New_Research_Deakin/venv/lib/python3.12/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/New_Research_Deakin/venv/lib/python3.12/threading.py:334\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys, getopt, os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import PauliFeatureMap, RealAmplitudes\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.circuit.library import QNNCircuit\n",
    "from qiskit_machine_learning.utils.loss_functions import L2Loss  # Qiskit ML loss, or use PyTorch's\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "root_folder = 'QNNC_hybrid'\n",
    "### Globals\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fixed feature sizes\n",
    "NUM_FEATURES = 3\n",
    "NUM_QUBITS = NUM_FEATURES\n",
    "NUM_TARGETS = 1\n",
    "\n",
    "# Quantum circuit parameters\n",
    "FEATURE_MAP_REPS_LIST = [1, 2, 3, 4, 5, 6]\n",
    "ANSATZ_REPS_LIST = [1, 2, 3, 4, 5, 6]\n",
    "ENTANGLEMENT_LIST = ['linear', 'full', 'circular']\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 30\n",
    "NUM_EPOCHS = 100  # Adjust as needed\n",
    "\n",
    "# K-fold cross-validation parameters\n",
    "N_REPEATS = 10\n",
    "TEST_SIZE = 1  # Leave-one-out cross-validation (LOOCV) is suggested since the sample size is too small\n",
    "\n",
    "def get_qnn_torch_model(entangle, feature_map_reps, ansatz_reps):\n",
    "    ### Feature Map, Ansatz, then QNN Constructor\n",
    "    # a. Feature Map: Encodes NUM_FEATURES into NUM_QUBITS\n",
    "    # ParameterVector for input features\n",
    "    input_params = ParameterVector(\"x\", NUM_FEATURES)\n",
    "\n",
    "    feature_map_template = PauliFeatureMap(\n",
    "        feature_dimension=NUM_FEATURES,  # This tells the template how many input parameters it structurally needs\n",
    "        reps=feature_map_reps,\n",
    "        entanglement=entangle\n",
    "    )\n",
    "\n",
    "    # Assign the *specific* input parameters from the vector to the template's parameter slots\n",
    "    # This creates a new circuit instance containing parameters ONLY from input_params (size NUM_FEATURES)\n",
    "    feature_map = feature_map_template.assign_parameters(input_params)\n",
    "    print(f\"Assigned feature map parameters: {feature_map.num_parameters}\")\n",
    "\n",
    "    # Create a template to find out how many parameters it needs structurally\n",
    "    ansatz_template = RealAmplitudes(NUM_QUBITS, reps=ansatz_reps, entanglement=entangle)\n",
    "    # ParameterVector for trainable weights - sized based on the template's structural parameters\n",
    "    num_ansatz_params = ansatz_template.num_parameters\n",
    "    weight_params = ParameterVector(\"θ\", num_ansatz_params)\n",
    "\n",
    "    # Create the ansatz circuit instance by assigning the weight parameters to the template\n",
    "    ansatz = ansatz_template.assign_parameters(weight_params)\n",
    "    print(f\"Assigned ansatz parameters: {ansatz.num_parameters}\")\n",
    "\n",
    "    # c. Combine into a full quantum circuit\n",
    "    qc = QNNCircuit(\n",
    "        feature_map=feature_map,\n",
    "        ansatz=ansatz_template,\n",
    "    )\n",
    "\n",
    "    # example 5.2 from Qiskit guide on binary classification\n",
    "    parity = lambda x: \"{:b}\".format(x).count(\"1\") % 2\n",
    "    output_shape = 2  # parity = 0, 1\n",
    "\n",
    "    sampler = Sampler()\n",
    "\n",
    "    qnn = SamplerQNN(\n",
    "        circuit=qc,\n",
    "        interpret=parity,\n",
    "        output_shape=output_shape,\n",
    "        sampler=sampler,\n",
    "        sparse=False,\n",
    "        input_gradients=False,  # Set to True if you need gradients w.r.t. inputs\n",
    "    )\n",
    "\n",
    "    # --- 4. TorchConnector ---\n",
    "    # Wrap the QNN into a PyTorch module\n",
    "    initial_weights = 0.01 * (2 * np.random.rand(qnn.num_weights) - 1)\n",
    "    qnn_torch_model = TorchConnector(qnn, initial_weights=torch.tensor(initial_weights, dtype=torch.float32))\n",
    "\n",
    "    return qnn_torch_model.to(device)\n",
    "\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, qnn_model):\n",
    "        super().__init__()\n",
    "        self.qnn = qnn_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.qnn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def prepare_dataset_k_fold(X, y, train_indices, test_indices):\n",
    "    # Separate train/test split\n",
    "    X_train_raw, X_test_raw = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    X_train = X_train_raw\n",
    "    X_test = X_test_raw\n",
    "    full_X = np.vstack([X_train, X_test])\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(full_X)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, y_train, X_test_scaled, y_test, None, None\n",
    "\n",
    "\n",
    "def get_arguments(argvs):\n",
    "    _entangle = ''\n",
    "    _feature_map_reps = ''\n",
    "    _ansatz_reps = ''\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argvs, \"h:e:f:a:\", [\"entangle=\", \"feature_map_reps=\", \"ansatz_reps=\"])\n",
    "    except getopt.GetoptError:\n",
    "        print(root_folder + '.py -e <entangle> -f <feature_map_reps> -a <ansatz_reps>')\n",
    "        sys.exit(2)\n",
    "    for opt, arg in opts:\n",
    "        if opt == '-h':\n",
    "            print(root_folder + '.py -e <entangle> -f <feature_map_reps> -a <ansatz_reps>')\n",
    "            sys.exit()\n",
    "        elif opt in (\"-e\", \"--entangle\"):\n",
    "            _entangle = arg\n",
    "        elif opt in (\"-f\", \"--feature_map_reps\"):\n",
    "            _feature_map_reps = int(arg)\n",
    "        elif opt in (\"-a\", \"--ansatz_reps\"):\n",
    "            _ansatz_reps = int(arg)\n",
    "    return _entangle, _feature_map_reps, _ansatz_reps\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    date = '06_08_25_1'\n",
    "    if not os.path.exists(f'{root_folder}/result'):\n",
    "        os.makedirs(f'{root_folder}/result')\n",
    "    if not os.path.exists(f'{root_folder}/logs'):\n",
    "        os.makedirs(f'{root_folder}/logs')\n",
    "\n",
    "    # Check if running in Jupyter\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        if get_ipython() is not None:\n",
    "            # Running in Jupyter, use default values\n",
    "            tmp1, tmp2, tmp3 = 'linear', 1, 1  # Default values\n",
    "        else:\n",
    "            # Running as script, parse arguments\n",
    "            tmp1, tmp2, tmp3 = get_arguments(sys.argv[1:])\n",
    "    except ImportError:\n",
    "        # No IPython, assume script and parse arguments\n",
    "        tmp1, tmp2, tmp3 = get_arguments(sys.argv[1:])\n",
    "\n",
    "    if tmp1 != '':\n",
    "        ENTANGLEMENT_LIST = [tmp1]\n",
    "    if tmp2 != '':\n",
    "        FEATURE_MAP_REPS_LIST = [tmp2]\n",
    "    if tmp3 != '':\n",
    "        ANSATZ_REPS_LIST = [tmp3]\n",
    "    print(f\"\\nFEATURE_MAP_REPS_LIST={FEATURE_MAP_REPS_LIST} \"\n",
    "          f\"ANSATZ_REPS_LIST={ANSATZ_REPS_LIST} ENTANGLEMENT_LIST={ENTANGLEMENT_LIST} date={date}\")\n",
    "    if len(FEATURE_MAP_REPS_LIST) == 1:\n",
    "        FEATURE_MAP_REPS_LIST_NAME = FEATURE_MAP_REPS_LIST[0]\n",
    "    else:\n",
    "        FEATURE_MAP_REPS_LIST_NAME = FEATURE_MAP_REPS_LIST\n",
    "    if len(ANSATZ_REPS_LIST) == 1:\n",
    "        ANSATZ_REPS_LIST_NAME = ANSATZ_REPS_LIST[0]\n",
    "    else:\n",
    "        ANSATZ_REPS_LIST_NAME = ANSATZ_REPS_LIST\n",
    "    if len(ENTANGLEMENT_LIST) == 1:\n",
    "        ENTANGLEMENT_LIST_NAME = ENTANGLEMENT_LIST[0]\n",
    "    else:\n",
    "        ENTANGLEMENT_LIST_NAME = ENTANGLEMENT_LIST\n",
    "    file_name = f'{root_folder}/result/FMR_{FEATURE_MAP_REPS_LIST_NAME}_AR_{ANSATZ_REPS_LIST_NAME}_E_{ENTANGLEMENT_LIST_NAME}_{date}.csv'\n",
    "\n",
    "    print(\"\\n--- Loading and Preprocessing Data ---\")\n",
    "\n",
    "    train_df = pd.read_csv(\"Training_Top3Features.csv\")\n",
    "    test_df = pd.read_csv(\"Testing_Top3Features.csv\")\n",
    "    X_train = train_df.drop('went_on_backorder', axis=1).values\n",
    "    y_train = train_df['went_on_backorder'].values\n",
    "    X_test = test_df.drop('went_on_backorder', axis=1).values\n",
    "    y_test = test_df['went_on_backorder'].values\n",
    "\n",
    "    # Combine train and test for k-fold\n",
    "    X = np.vstack([X_train, X_test])\n",
    "    y = np.hstack([y_train, y_test])\n",
    "\n",
    "    print('Total number of data: ', X.shape[0])\n",
    "    rkf = RepeatedKFold(n_splits=X.shape[0] // TEST_SIZE, n_repeats=N_REPEATS)\n",
    "\n",
    "    df = pd.DataFrame(columns=['entanglement', 'feature_map_reps', 'ansatz_reps',\n",
    "                               'element test', 'actual test', 'predicted test',\n",
    "                               'element train', 'actual train', 'predicted train',\n",
    "                               ])\n",
    "    i = 0\n",
    "\n",
    "    LOSS = nn.CrossEntropyLoss()  # use torch.long\n",
    "\n",
    "    print(\"\\n--- Start K-Fold Loop ---\")\n",
    "\n",
    "    for train_indices, test_indices in rkf.split(X):\n",
    "        X_train_scaled, y_train, X_test_scaled, y_test, element_test, element_train = prepare_dataset_k_fold(X, y, train_indices, test_indices)\n",
    "\n",
    "        X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "        y_train_t = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "        X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "        y_test_t = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "        # Create DataLoaders\n",
    "        train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        print(f\"Training data shape: X_train_t: {X_train_t.shape}, y_train_t: {y_train_t.shape}\")\n",
    "        print(f\"Testing data shape: X_test_t: {X_test_t.shape}, y_test_t: {y_test_t.shape}\")\n",
    "\n",
    "        for entanglement in ENTANGLEMENT_LIST:\n",
    "            for feature_map_reps in FEATURE_MAP_REPS_LIST:\n",
    "                for ansatz_reps in ANSATZ_REPS_LIST:\n",
    "                    # Build model\n",
    "                    model = HybridModel(get_qnn_torch_model(entangle=entanglement,\n",
    "                                                            feature_map_reps=feature_map_reps,\n",
    "                                                            ansatz_reps=ansatz_reps)).to(device)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "                    print(f\"\\n--- Starting Training {i}th---\")\n",
    "                    train_losses = []\n",
    "                    test_losses = []\n",
    "\n",
    "                    for epoch in range(NUM_EPOCHS):\n",
    "                        # Training phase\n",
    "                        model.train()\n",
    "                        running_loss = 0.0\n",
    "                        for batch_X, batch_y in train_loader:\n",
    "                            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                            optimizer.zero_grad()  # Clear gradients\n",
    "                            outputs = model(batch_X)  # Forward pass\n",
    "                            loss = LOSS(outputs, batch_y)  # Calculate loss\n",
    "                            loss.backward()  # Backward pass (compute gradients)\n",
    "                            optimizer.step()  # Update weights\n",
    "                            running_loss += loss.item() * batch_X.size(0)\n",
    "\n",
    "                        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "                        train_losses.append(epoch_loss)\n",
    "\n",
    "                        # Validation/Test phase\n",
    "                        model.eval()\n",
    "                        test_loss = 0.0\n",
    "                        with torch.no_grad():  # Disable gradient calculations\n",
    "                            for batch_X_test, batch_y_test in test_loader:\n",
    "                                batch_X_test, batch_y_test = batch_X_test.to(device), batch_y_test.to(device)\n",
    "                                outputs_test = model(batch_X_test)\n",
    "                                loss_test = LOSS(outputs_test, batch_y_test)\n",
    "                                test_loss += loss_test.item() * batch_X_test.size(0)\n",
    "\n",
    "                        epoch_test_loss = test_loss / len(test_loader.dataset)\n",
    "                        test_losses.append(epoch_test_loss)\n",
    "\n",
    "                        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Train Loss: {epoch_loss:.4f}, Test Loss: {epoch_test_loss:.4f}\")\n",
    "\n",
    "                    print(\"--- Training Finished ---\")\n",
    "\n",
    "                    # --- Evaluation on Test and Training Set ---\n",
    "                    model.eval()\n",
    "                    all_preds = []\n",
    "                    all_targets = []\n",
    "                    all_preds_train = []\n",
    "                    all_targets_train = []\n",
    "                    with torch.no_grad():\n",
    "                        for batch_X_test, batch_y_test in test_loader:\n",
    "                            batch_X_test = batch_X_test.to(device)\n",
    "                            outputs_test = model(batch_X_test)\n",
    "                            all_preds.extend(outputs_test.cpu().numpy())\n",
    "                            all_targets.extend(batch_y_test.cpu().numpy())\n",
    "                        for batch_X_train, batch_y_train in train_loader:\n",
    "                            batch_X_train = batch_X_train.to(device)\n",
    "                            outputs_train = model(batch_X_train)\n",
    "                            all_preds_train.extend(outputs_train.cpu().numpy())\n",
    "                            all_targets_train.extend(batch_y_train.cpu().numpy())\n",
    "\n",
    "                    all_preds = np.array(all_preds)\n",
    "                    all_targets = np.array(all_targets)\n",
    "                    all_preds_temp = []\n",
    "                    for item in all_preds:\n",
    "                        if item[1] > item[0]:\n",
    "                            all_preds_temp.append(1)\n",
    "                        else:\n",
    "                            all_preds_temp.append(0)\n",
    "                    all_preds = np.array(all_preds_temp)\n",
    "\n",
    "                    all_preds_train = np.array(all_preds_train)\n",
    "                    all_targets_train = np.array(all_targets_train)\n",
    "                    all_preds_temp = []\n",
    "                    for item in all_preds_train:\n",
    "                        if item[1] > item[0]:\n",
    "                            all_preds_temp.append(1)\n",
    "                        else:\n",
    "                            all_preds_temp.append(0)\n",
    "                    all_preds_train = np.array(all_preds_temp)\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    train_accuracy = accuracy_score(all_targets_train, all_preds_train)\n",
    "                    test_accuracy = accuracy_score(all_targets, all_preds)\n",
    "                    train_f1 = f1_score(all_targets_train, all_preds_train, zero_division=0)\n",
    "                    test_f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
    "                    train_recall = recall_score(all_targets_train, all_preds_train, zero_division=0)\n",
    "                    test_recall = recall_score(all_targets, all_preds, zero_division=0)\n",
    "                    train_precision = precision_score(all_targets_train, all_preds_train, zero_division=0)\n",
    "                    test_precision = precision_score(all_targets, all_preds, zero_division=0)\n",
    "                    train_cm = confusion_matrix(all_targets_train, all_preds_train)\n",
    "                    test_cm = confusion_matrix(all_targets, all_preds, labels=[0, 1])\n",
    "\n",
    "                    print(f\"\\n--- Metrics for entanglement: {entanglement}, feature_map_reps: {feature_map_reps}, ansatz_reps: {ansatz_reps} ---\")\n",
    "                    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "                    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "                    print(f\"Train F1 Score: {train_f1:.4f}\")\n",
    "                    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "                    print(f\"Train Recall: {train_recall:.4f}\")\n",
    "                    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "                    print(f\"Train Precision: {train_precision:.4f}\")\n",
    "                    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "                    print(f\"Train Confusion Matrix:\\n{train_cm}\")\n",
    "                    print(f\"Test Confusion Matrix:\\n{test_cm}\")\n",
    "\n",
    "                    print(torch.cuda.memory_allocated())\n",
    "                    print(torch.cuda.max_memory_allocated())\n",
    "\n",
    "                    print(f\"\\n--- Done for entanglement: {entanglement}, feature_map_reps: {feature_map_reps}, ansatz_reps: {ansatz_reps} ---\")\n",
    "\n",
    "                    # Add to dataframe\n",
    "                    new_row = {'entanglement': entanglement,\n",
    "                               'feature_map_reps': feature_map_reps,\n",
    "                               'ansatz_reps': ansatz_reps,\n",
    "                               'element test': element_test if element_test is not None else 'N/A',\n",
    "                               'actual test': np.array(all_targets).flatten(),\n",
    "                               'predicted test': np.array(all_preds).flatten(),\n",
    "                               'element train': element_train if element_train is not None else 'N/A',\n",
    "                               'actual train': np.array(all_targets_train).flatten(),\n",
    "                               'predicted train': np.array(all_preds_train).flatten(),\n",
    "                               }\n",
    "                    df.loc[len(df)] = new_row\n",
    "                    with np.printoptions(linewidth=10000):\n",
    "                        df.to_csv(file_name, index=False)\n",
    "        i += 1\n",
    "    df.at[0, \"info\"] = [f\"DATASET: Training_Top3Features.csv, Testing_Top3Features.csv, LEARNING_RATE = {LEARNING_RATE}, \"\n",
    "                        f\"BATCH_SIZE = {BATCH_SIZE}, NUM_EPOCHS = {NUM_EPOCHS}, LOSS: {LOSS}\"]\n",
    "    with np.printoptions(linewidth=10000):\n",
    "        df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e58d02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "FEATURE_MAP_REPS_LIST=[1] ANSATZ_REPS_LIST=[1] ENTANGLEMENT_LIST=['linear'] date=06_08_25_1\n",
      "\n",
      "--- Loading and Preprocessing Data ---\n",
      "Training data size:  10000\n",
      "Testing data size:  4000\n",
      "\n",
      "--- Start Training Loop ---\n",
      "Training data shape: X_train_t: torch.Size([10000, 3]), y_train_t: torch.Size([10000])\n",
      "Testing data shape: X_test_t: torch.Size([4000, 3]), y_test_t: torch.Size([4000])\n",
      "Assigned feature map parameters: 3\n",
      "Assigned ansatz parameters: 6\n",
      "\n",
      "--- Starting Training 1th ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/713294462.py:70: DeprecationWarning: The class ``qiskit.primitives.sampler.Sampler`` is deprecated as of qiskit 1.2. It will be removed no earlier than 3 months after the release date. All implementations of the `BaseSamplerV1` interface have been deprecated in favor of their V2 counterparts. The V2 alternative for the `Sampler` class is `StatevectorSampler`.\n",
      "  sampler = Sampler()\n",
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/713294462.py:71: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.\n",
      "  qnn = SamplerQNN(\n",
      "/Users/pawanpahune/New_Research_Deakin/venv/lib/python3.12/site-packages/qiskit_machine_learning/connectors/torch_connector.py:378: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self._weights.data = torch.tensor(initial_weights, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.6687, Test Loss: 0.6664\n",
      "Epoch 2/100, Train Loss: 0.6613, Test Loss: 0.6651\n",
      "Epoch 3/100, Train Loss: 0.6615, Test Loss: 0.6652\n",
      "Epoch 4/100, Train Loss: 0.6617, Test Loss: 0.6652\n",
      "Epoch 5/100, Train Loss: 0.6614, Test Loss: 0.6650\n",
      "Epoch 6/100, Train Loss: 0.6614, Test Loss: 0.6651\n",
      "Epoch 7/100, Train Loss: 0.6614, Test Loss: 0.6659\n",
      "Epoch 8/100, Train Loss: 0.6614, Test Loss: 0.6656\n",
      "Epoch 9/100, Train Loss: 0.6615, Test Loss: 0.6657\n",
      "Epoch 10/100, Train Loss: 0.6613, Test Loss: 0.6654\n",
      "Epoch 11/100, Train Loss: 0.6613, Test Loss: 0.6654\n",
      "Epoch 12/100, Train Loss: 0.6614, Test Loss: 0.6653\n",
      "Epoch 13/100, Train Loss: 0.6615, Test Loss: 0.6651\n",
      "Epoch 14/100, Train Loss: 0.6612, Test Loss: 0.6650\n",
      "Epoch 15/100, Train Loss: 0.6614, Test Loss: 0.6649\n",
      "Epoch 16/100, Train Loss: 0.6614, Test Loss: 0.6659\n",
      "Epoch 17/100, Train Loss: 0.6614, Test Loss: 0.6656\n",
      "Epoch 18/100, Train Loss: 0.6612, Test Loss: 0.6660\n",
      "Epoch 19/100, Train Loss: 0.6615, Test Loss: 0.6661\n",
      "Epoch 20/100, Train Loss: 0.6614, Test Loss: 0.6650\n",
      "Epoch 21/100, Train Loss: 0.6613, Test Loss: 0.6652\n",
      "Epoch 22/100, Train Loss: 0.6614, Test Loss: 0.6651\n",
      "Epoch 23/100, Train Loss: 0.6613, Test Loss: 0.6653\n",
      "Epoch 24/100, Train Loss: 0.6614, Test Loss: 0.6653\n",
      "Epoch 25/100, Train Loss: 0.6614, Test Loss: 0.6654\n",
      "Epoch 26/100, Train Loss: 0.6614, Test Loss: 0.6654\n",
      "Epoch 27/100, Train Loss: 0.6614, Test Loss: 0.6654\n",
      "Epoch 28/100, Train Loss: 0.6613, Test Loss: 0.6650\n",
      "Epoch 29/100, Train Loss: 0.6614, Test Loss: 0.6650\n",
      "Epoch 30/100, Train Loss: 0.6615, Test Loss: 0.6651\n",
      "Epoch 31/100, Train Loss: 0.6614, Test Loss: 0.6651\n",
      "Epoch 32/100, Train Loss: 0.6614, Test Loss: 0.6651\n",
      "Epoch 33/100, Train Loss: 0.6614, Test Loss: 0.6651\n",
      "Epoch 34/100, Train Loss: 0.6615, Test Loss: 0.6653\n",
      "Epoch 35/100, Train Loss: 0.6613, Test Loss: 0.6651\n",
      "Epoch 36/100, Train Loss: 0.6612, Test Loss: 0.6656\n",
      "Epoch 37/100, Train Loss: 0.6615, Test Loss: 0.6650\n",
      "Epoch 38/100, Train Loss: 0.6614, Test Loss: 0.6651\n",
      "Epoch 39/100, Train Loss: 0.6615, Test Loss: 0.6656\n",
      "Epoch 40/100, Train Loss: 0.6615, Test Loss: 0.6653\n",
      "Epoch 41/100, Train Loss: 0.6613, Test Loss: 0.6654\n",
      "Epoch 42/100, Train Loss: 0.6613, Test Loss: 0.6659\n",
      "Epoch 43/100, Train Loss: 0.6615, Test Loss: 0.6651\n",
      "Epoch 44/100, Train Loss: 0.6614, Test Loss: 0.6650\n",
      "Epoch 45/100, Train Loss: 0.6613, Test Loss: 0.6650\n",
      "Epoch 46/100, Train Loss: 0.6614, Test Loss: 0.6653\n",
      "Epoch 47/100, Train Loss: 0.6614, Test Loss: 0.6649\n",
      "Epoch 48/100, Train Loss: 0.6614, Test Loss: 0.6653\n",
      "Epoch 49/100, Train Loss: 0.6613, Test Loss: 0.6662\n",
      "Epoch 50/100, Train Loss: 0.6615, Test Loss: 0.6652\n",
      "Epoch 51/100, Train Loss: 0.6614, Test Loss: 0.6659\n",
      "Epoch 52/100, Train Loss: 0.6615, Test Loss: 0.6651\n",
      "Epoch 53/100, Train Loss: 0.6613, Test Loss: 0.6655\n",
      "Epoch 54/100, Train Loss: 0.6614, Test Loss: 0.6653\n",
      "Epoch 55/100, Train Loss: 0.6613, Test Loss: 0.6651\n",
      "Epoch 56/100, Train Loss: 0.6614, Test Loss: 0.6659\n",
      "Epoch 57/100, Train Loss: 0.6614, Test Loss: 0.6655\n",
      "Epoch 58/100, Train Loss: 0.6614, Test Loss: 0.6652\n",
      "Epoch 59/100, Train Loss: 0.6613, Test Loss: 0.6651\n",
      "Epoch 60/100, Train Loss: 0.6615, Test Loss: 0.6650\n",
      "Epoch 61/100, Train Loss: 0.6615, Test Loss: 0.6650\n",
      "Epoch 62/100, Train Loss: 0.6613, Test Loss: 0.6652\n",
      "Epoch 63/100, Train Loss: 0.6614, Test Loss: 0.6648\n",
      "Epoch 64/100, Train Loss: 0.6613, Test Loss: 0.6655\n",
      "Epoch 65/100, Train Loss: 0.6614, Test Loss: 0.6655\n",
      "Epoch 66/100, Train Loss: 0.6614, Test Loss: 0.6653\n",
      "Epoch 67/100, Train Loss: 0.6614, Test Loss: 0.6653\n",
      "Epoch 68/100, Train Loss: 0.6613, Test Loss: 0.6650\n",
      "Epoch 69/100, Train Loss: 0.6615, Test Loss: 0.6654\n",
      "Epoch 70/100, Train Loss: 0.6614, Test Loss: 0.6661\n",
      "Epoch 71/100, Train Loss: 0.6614, Test Loss: 0.6655\n",
      "Epoch 72/100, Train Loss: 0.6614, Test Loss: 0.6651\n",
      "Epoch 73/100, Train Loss: 0.6615, Test Loss: 0.6649\n",
      "Epoch 74/100, Train Loss: 0.6614, Test Loss: 0.6651\n",
      "Epoch 75/100, Train Loss: 0.6613, Test Loss: 0.6652\n",
      "Epoch 76/100, Train Loss: 0.6614, Test Loss: 0.6654\n",
      "Epoch 77/100, Train Loss: 0.6614, Test Loss: 0.6657\n",
      "Epoch 78/100, Train Loss: 0.6613, Test Loss: 0.6651\n",
      "Epoch 79/100, Train Loss: 0.6613, Test Loss: 0.6655\n",
      "Epoch 80/100, Train Loss: 0.6616, Test Loss: 0.6648\n",
      "Epoch 81/100, Train Loss: 0.6614, Test Loss: 0.6652\n",
      "Epoch 82/100, Train Loss: 0.6613, Test Loss: 0.6656\n",
      "Epoch 83/100, Train Loss: 0.6614, Test Loss: 0.6651\n",
      "Epoch 84/100, Train Loss: 0.6613, Test Loss: 0.6649\n",
      "Epoch 85/100, Train Loss: 0.6612, Test Loss: 0.6652\n",
      "Epoch 86/100, Train Loss: 0.6614, Test Loss: 0.6656\n",
      "Epoch 87/100, Train Loss: 0.6615, Test Loss: 0.6650\n",
      "Epoch 88/100, Train Loss: 0.6614, Test Loss: 0.6652\n",
      "Epoch 89/100, Train Loss: 0.6614, Test Loss: 0.6648\n",
      "Epoch 90/100, Train Loss: 0.6615, Test Loss: 0.6652\n",
      "Epoch 91/100, Train Loss: 0.6612, Test Loss: 0.6654\n",
      "Epoch 92/100, Train Loss: 0.6614, Test Loss: 0.6651\n",
      "Epoch 93/100, Train Loss: 0.6613, Test Loss: 0.6652\n",
      "Epoch 94/100, Train Loss: 0.6614, Test Loss: 0.6655\n",
      "Epoch 95/100, Train Loss: 0.6614, Test Loss: 0.6652\n",
      "Epoch 96/100, Train Loss: 0.6614, Test Loss: 0.6653\n",
      "Epoch 97/100, Train Loss: 0.6613, Test Loss: 0.6649\n",
      "Epoch 98/100, Train Loss: 0.6613, Test Loss: 0.6653\n",
      "Epoch 99/100, Train Loss: 0.6614, Test Loss: 0.6654\n",
      "Epoch 100/100, Train Loss: 0.6614, Test Loss: 0.6648\n",
      "--- Training Finished ---\n",
      "\n",
      "--- Metrics for entanglement: linear, feature_map_reps: 1, ansatz_reps: 1 ---\n",
      "Train Accuracy: 0.5819\n",
      "Test Accuracy: 0.5700\n",
      "Train F1 Score: 0.6400\n",
      "Test F1 Score: 0.6195\n",
      "Train Recall: 0.7434\n",
      "Test Recall: 0.7000\n",
      "Train Precision: 0.5619\n",
      "Test Precision: 0.5556\n",
      "Train Confusion Matrix:\n",
      "[[2102 2898]\n",
      " [1283 3717]]\n",
      "Test Confusion Matrix:\n",
      "[[ 880 1120]\n",
      " [ 600 1400]]\n",
      "0\n",
      "0\n",
      "\n",
      "--- Done for entanglement: linear, feature_map_reps: 1, ansatz_reps: 1 ---\n"
     ]
    }
   ],
   "source": [
    "import sys, getopt, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import PauliFeatureMap, RealAmplitudes\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.circuit.library import QNNCircuit\n",
    "from qiskit_machine_learning.utils.loss_functions import L2Loss\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "root_folder = 'QNNC_hybrid'\n",
    "### Globals\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fixed feature sizes\n",
    "NUM_FEATURES = 3\n",
    "NUM_QUBITS = NUM_FEATURES\n",
    "NUM_TARGETS = 1\n",
    "\n",
    "# Quantum circuit parameters\n",
    "FEATURE_MAP_REPS_LIST = [1, 2, 3, 4, 5, 6]\n",
    "ANSATZ_REPS_LIST = [1, 2, 3, 4, 5, 6]\n",
    "ENTANGLEMENT_LIST = ['linear', 'full', 'circular']\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 30\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Training repetition parameters\n",
    "N_REPEATS = 1\n",
    "\n",
    "def get_qnn_torch_model(entangle, feature_map_reps, ansatz_reps):\n",
    "    input_params = ParameterVector(\"x\", NUM_FEATURES)\n",
    "    feature_map_template = PauliFeatureMap(\n",
    "        feature_dimension=NUM_FEATURES,\n",
    "        reps=feature_map_reps,\n",
    "        entanglement=entangle\n",
    "    )\n",
    "    feature_map = feature_map_template.assign_parameters(input_params)\n",
    "    print(f\"Assigned feature map parameters: {feature_map.num_parameters}\")\n",
    "    ansatz_template = RealAmplitudes(NUM_QUBITS, reps=ansatz_reps, entanglement=entangle)\n",
    "    num_ansatz_params = ansatz_template.num_parameters\n",
    "    weight_params = ParameterVector(\"θ\", num_ansatz_params)\n",
    "    ansatz = ansatz_template.assign_parameters(weight_params)\n",
    "    print(f\"Assigned ansatz parameters: {ansatz.num_parameters}\")\n",
    "    qc = QNNCircuit(\n",
    "        feature_map=feature_map,\n",
    "        ansatz=ansatz_template,\n",
    "    )\n",
    "    parity = lambda x: \"{:b}\".format(x).count(\"1\") % 2\n",
    "    output_shape = 2\n",
    "    sampler = Sampler()\n",
    "    qnn = SamplerQNN(\n",
    "        circuit=qc,\n",
    "        interpret=parity,\n",
    "        output_shape=output_shape,\n",
    "        sampler=sampler,\n",
    "        sparse=False,\n",
    "        input_gradients=False,\n",
    "    )\n",
    "    initial_weights = 0.01 * (2 * np.random.rand(qnn.num_weights) - 1)\n",
    "    qnn_torch_model = TorchConnector(qnn, initial_weights=torch.tensor(initial_weights, dtype=torch.float32))\n",
    "    return qnn_torch_model.to(device)\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, qnn_model):\n",
    "        super().__init__()\n",
    "        self.qnn = qnn_model\n",
    "    def forward(self, x):\n",
    "        x = self.qnn(x)\n",
    "        return x\n",
    "\n",
    "def prepare_dataset_k_fold(X_train, y_train, X_test, y_test):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(np.vstack([X_train, X_test]))\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, y_train, X_test_scaled, y_test, None, None\n",
    "\n",
    "def get_arguments(argvs):\n",
    "    _entangle = ''\n",
    "    _feature_map_reps = ''\n",
    "    _ansatz_reps = ''\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argvs, \"h:e:f:a:\", [\"entangle=\", \"feature_map_reps=\", \"ansatz_reps=\"])\n",
    "    except getopt.GetoptError:\n",
    "        print(root_folder + '.py -e <entangle> -f <feature_map_reps> -a <ansatz_reps>')\n",
    "        sys.exit(2)\n",
    "    for opt, arg in opts:\n",
    "        if opt == '-h':\n",
    "            print(root_folder + '.py -e <entangle> -f <feature_map_reps> -a <ansatz_reps>')\n",
    "            sys.exit()\n",
    "        elif opt in (\"-e\", \"--entangle\"):\n",
    "            _entangle = arg\n",
    "        elif opt in (\"-f\", \"--feature_map_reps\"):\n",
    "            _feature_map_reps = int(arg)\n",
    "        elif opt in (\"-a\", \"--ansatz_reps\"):\n",
    "            _ansatz_reps = int(arg)\n",
    "    return _entangle, _feature_map_reps, _ansatz_reps\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    date = '06_08_25_1'\n",
    "    if not os.path.exists(f'{root_folder}/result'):\n",
    "        os.makedirs(f'{root_folder}/result')\n",
    "    if not os.path.exists(f'{root_folder}/logs'):\n",
    "        os.makedirs(f'{root_folder}/logs')\n",
    "    if not os.path.exists(f'{root_folder}/figures'):\n",
    "        os.makedirs(f'{root_folder}/figures')\n",
    "\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        if get_ipython() is not None:\n",
    "            tmp1, tmp2, tmp3 = 'linear', 1, 1\n",
    "        else:\n",
    "            tmp1, tmp2, tmp3 = get_arguments(sys.argv[1:])\n",
    "    except ImportError:\n",
    "        tmp1, tmp2, tmp3 = get_arguments(sys.argv[1:])\n",
    "\n",
    "    if tmp1 != '':\n",
    "        ENTANGLEMENT_LIST = [tmp1]\n",
    "    if tmp2 != '':\n",
    "        FEATURE_MAP_REPS_LIST = [tmp2]\n",
    "    if tmp3 != '':\n",
    "        ANSATZ_REPS_LIST = [tmp3]\n",
    "    print(f\"\\nFEATURE_MAP_REPS_LIST={FEATURE_MAP_REPS_LIST} \"\n",
    "          f\"ANSATZ_REPS_LIST={ANSATZ_REPS_LIST} ENTANGLEMENT_LIST={ENTANGLEMENT_LIST} date={date}\")\n",
    "    if len(FEATURE_MAP_REPS_LIST) == 1:\n",
    "        FEATURE_MAP_REPS_LIST_NAME = FEATURE_MAP_REPS_LIST[0]\n",
    "    else:\n",
    "        FEATURE_MAP_REPS_LIST_NAME = FEATURE_MAP_REPS_LIST\n",
    "    if len(ANSATZ_REPS_LIST) == 1:\n",
    "        ANSATZ_REPS_LIST_NAME = ANSATZ_REPS_LIST[0]\n",
    "    else:\n",
    "        ANSATZ_REPS_LIST_NAME = ANSATZ_REPS_LIST\n",
    "    if len(ENTANGLEMENT_LIST) == 1:\n",
    "        ENTANGLEMENT_LIST_NAME = ENTANGLEMENT_LIST[0]\n",
    "    else:\n",
    "        ENTANGLEMENT_LIST_NAME = ENTANGLEMENT_LIST\n",
    "    file_name = f'{root_folder}/result/FMR_{FEATURE_MAP_REPS_LIST_NAME}_AR_{ANSATZ_REPS_LIST_NAME}_E_{ENTANGLEMENT_LIST_NAME}_{date}.csv'\n",
    "\n",
    "    print(\"\\n--- Loading and Preprocessing Data ---\")\n",
    "\n",
    "    train_df = pd.read_csv(\"Training_Top3Features.csv\")\n",
    "    test_df = pd.read_csv(\"Testing_Top3Features.csv\")\n",
    "    X_train = train_df.drop('went_on_backorder', axis=1).values\n",
    "    y_train = train_df['went_on_backorder'].values\n",
    "    X_test = test_df.drop('went_on_backorder', axis=1).values\n",
    "    y_test = test_df['went_on_backorder'].values\n",
    "\n",
    "    print('Training data size: ', X_train.shape[0])\n",
    "    print('Testing data size: ', X_test.shape[0])\n",
    "\n",
    "    df = pd.DataFrame(columns=['entanglement', 'feature_map_reps', 'ansatz_reps',\n",
    "                               'element test', 'actual test', 'predicted test',\n",
    "                               'element train', 'actual train', 'predicted train'])\n",
    "\n",
    "    LOSS = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"\\n--- Start Training Loop ---\")\n",
    "\n",
    "    for i in range(N_REPEATS):\n",
    "        X_train_scaled, y_train, X_test_scaled, y_test, element_test, element_train = prepare_dataset_k_fold(X_train, y_train, X_test, y_test)\n",
    "\n",
    "        X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "        y_train_t = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "        X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "        y_test_t = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        print(f\"Training data shape: X_train_t: {X_train_t.shape}, y_train_t: {y_train_t.shape}\")\n",
    "        print(f\"Testing data shape: X_test_t: {X_test_t.shape}, y_test_t: {y_test_t.shape}\")\n",
    "\n",
    "        for entanglement in ENTANGLEMENT_LIST:\n",
    "            for feature_map_reps in FEATURE_MAP_REPS_LIST:\n",
    "                for ansatz_reps in ANSATZ_REPS_LIST:\n",
    "                    model = HybridModel(get_qnn_torch_model(entangle=entanglement,\n",
    "                                                            feature_map_reps=feature_map_reps,\n",
    "                                                            ansatz_reps=ansatz_reps)).to(device)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "                    print(f\"\\n--- Starting Training {i+1}th ---\")\n",
    "                    train_losses = []\n",
    "                    test_losses = []\n",
    "\n",
    "                    for epoch in range(NUM_EPOCHS):\n",
    "                        model.train()\n",
    "                        running_loss = 0.0\n",
    "                        for batch_X, batch_y in train_loader:\n",
    "                            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            outputs = model(batch_X)\n",
    "                            loss = LOSS(outputs, batch_y)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            running_loss += loss.item() * batch_X.size(0)\n",
    "\n",
    "                        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "                        train_losses.append(epoch_loss)\n",
    "\n",
    "                        model.eval()\n",
    "                        test_loss = 0.0\n",
    "                        with torch.no_grad():\n",
    "                            for batch_X_test, batch_y_test in test_loader:\n",
    "                                batch_X_test, batch_y_test = batch_X_test.to(device), batch_y_test.to(device)\n",
    "                                outputs_test = model(batch_X_test)\n",
    "                                loss_test = LOSS(outputs_test, batch_y_test)\n",
    "                                test_loss += loss_test.item() * batch_X_test.size(0)\n",
    "\n",
    "                        epoch_test_loss = test_loss / len(test_loader.dataset)\n",
    "                        test_losses.append(epoch_test_loss)\n",
    "\n",
    "                        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Train Loss: {epoch_loss:.4f}, Test Loss: {epoch_test_loss:.4f}\")\n",
    "\n",
    "                    print(\"--- Training Finished ---\")\n",
    "\n",
    "                    model.eval()\n",
    "                    all_preds = []\n",
    "                    all_targets = []\n",
    "                    all_preds_train = []\n",
    "                    all_targets_train = []\n",
    "                    with torch.no_grad():\n",
    "                        for batch_X_test, batch_y_test in test_loader:\n",
    "                            batch_X_test = batch_X_test.to(device)\n",
    "                            outputs_test = model(batch_X_test)\n",
    "                            all_preds.extend(outputs_test.cpu().numpy())\n",
    "                            all_targets.extend(batch_y_test.cpu().numpy())\n",
    "                        for batch_X_train, batch_y_train in train_loader:\n",
    "                            batch_X_train = batch_X_train.to(device)\n",
    "                            outputs_train = model(batch_X_train)\n",
    "                            all_preds_train.extend(outputs_train.cpu().numpy())\n",
    "                            all_targets_train.extend(batch_y_train.cpu().numpy())\n",
    "\n",
    "                    all_preds = np.array(all_preds)\n",
    "                    all_targets = np.array(all_targets)\n",
    "                    all_preds_temp = []\n",
    "                    for item in all_preds:\n",
    "                        if item[1] > item[0]:\n",
    "                            all_preds_temp.append(1)\n",
    "                        else:\n",
    "                            all_preds_temp.append(0)\n",
    "                    all_preds = np.array(all_preds_temp)\n",
    "\n",
    "                    all_preds_train = np.array(all_preds_train)\n",
    "                    all_targets_train = np.array(all_targets_train)\n",
    "                    all_preds_temp = []\n",
    "                    for item in all_preds_train:\n",
    "                        if item[1] > item[0]:\n",
    "                            all_preds_temp.append(1)\n",
    "                        else:\n",
    "                            all_preds_temp.append(0)\n",
    "                    all_preds_train = np.array(all_preds_temp)\n",
    "\n",
    "                    train_accuracy = accuracy_score(all_targets_train, all_preds_train)\n",
    "                    test_accuracy = accuracy_score(all_targets, all_preds)\n",
    "                    train_f1 = f1_score(all_targets_train, all_preds_train, zero_division=0)\n",
    "                    test_f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
    "                    train_recall = recall_score(all_targets_train, all_preds_train, zero_division=0)\n",
    "                    test_recall = recall_score(all_targets, all_preds, zero_division=0)\n",
    "                    train_precision = precision_score(all_targets_train, all_preds_train, zero_division=0)\n",
    "                    test_precision = precision_score(all_targets, all_preds, zero_division=0)\n",
    "                    train_cm = confusion_matrix(all_targets_train, all_preds_train)\n",
    "                    test_cm = confusion_matrix(all_targets, all_preds, labels=[0, 1])\n",
    "\n",
    "                    print(f\"\\n--- Metrics for entanglement: {entanglement}, feature_map_reps: {feature_map_reps}, ansatz_reps: {ansatz_reps} ---\")\n",
    "                    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "                    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "                    print(f\"Train F1 Score: {train_f1:.4f}\")\n",
    "                    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "                    print(f\"Train Recall: {train_recall:.4f}\")\n",
    "                    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "                    print(f\"Train Precision: {train_precision:.4f}\")\n",
    "                    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "                    print(f\"Train Confusion Matrix:\\n{train_cm}\")\n",
    "                    print(f\"Test Confusion Matrix:\\n{test_cm}\")\n",
    "\n",
    "                    plt.figure(figsize=(8, 6))\n",
    "                    sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "                    plt.title(f'Train Confusion Matrix\\nEntanglement: {entanglement}, FMR: {feature_map_reps}, AR: {ansatz_reps}')\n",
    "                    plt.xlabel('Predicted')\n",
    "                    plt.ylabel('Actual')\n",
    "                    plt.savefig(f'{root_folder}/figures/train_cm_{entanglement}_fmr{feature_map_reps}_ar{ansatz_reps}_rep{i+1}_{date}.png')\n",
    "                    plt.close()\n",
    "\n",
    "                    plt.figure(figsize=(8, 6))\n",
    "                    sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "                    plt.title(f'Test Confusion Matrix\\nEntanglement: {entanglement}, FMR: {feature_map_reps}, AR: {ansatz_reps}')\n",
    "                    plt.xlabel('Predicted')\n",
    "                    plt.ylabel('Actual')\n",
    "                    plt.savefig(f'{root_folder}/figures/test_cm_{entanglement}_fmr{feature_map_reps}_ar{ansatz_reps}_rep{i+1}_{date}.png')\n",
    "                    plt.close()\n",
    "\n",
    "                    print(torch.cuda.memory_allocated())\n",
    "                    print(torch.cuda.max_memory_allocated())\n",
    "\n",
    "                    print(f\"\\n--- Done for entanglement: {entanglement}, feature_map_reps: {feature_map_reps}, ansatz_reps: {ansatz_reps} ---\")\n",
    "\n",
    "                    new_row = {'entanglement': entanglement,\n",
    "                               'feature_map_reps': feature_map_reps,\n",
    "                               'ansatz_reps': ansatz_reps,\n",
    "                               'element test': element_test if element_test is not None else 'N/A',\n",
    "                               'actual test': np.array(all_targets).flatten(),\n",
    "                               'predicted test': np.array(all_preds).flatten(),\n",
    "                               'element train': element_train if element_train is not None else 'N/A',\n",
    "                               'actual train': np.array(all_targets_train).flatten(),\n",
    "                               'predicted train': np.array(all_preds_train).flatten()}\n",
    "                    df.loc[len(df)] = new_row\n",
    "                    with np.printoptions(linewidth=10000):\n",
    "                        df.to_csv(file_name, index=False)\n",
    "\n",
    "    df.at[0, \"info\"] = [f\"DATASET: Training_Top3Features.csv, Testing_Top3Features.csv, LEARNING_RATE = {LEARNING_RATE}, \"\n",
    "                        f\"BATCH_SIZE = {BATCH_SIZE}, NUM_EPOCHS = {NUM_EPOCHS}, LOSS: {LOSS}\"]\n",
    "    with np.printoptions(linewidth=10000):\n",
    "        df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea21088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "--- Loading and Preprocessing Data ---\n",
      "Train: (10000, 3), Test: (4000, 3)\n",
      "\n",
      "--- Training with entanglement = linear ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/1950909632.py:57: DeprecationWarning: The class ``qiskit.primitives.sampler.Sampler`` is deprecated as of qiskit 1.2. It will be removed no earlier than 3 months after the release date. All implementations of the `BaseSamplerV1` interface have been deprecated in favor of their V2 counterparts. The V2 alternative for the `Sampler` class is `StatevectorSampler`.\n",
      "  sampler = Sampler()\n",
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/1950909632.py:58: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.\n",
      "  qnn = SamplerQNN(\n",
      "/Users/pawanpahune/New_Research_Deakin/venv/lib/python3.12/site-packages/qiskit_machine_learning/connectors/torch_connector.py:378: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self._weights.data = torch.tensor(initial_weights, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.6849\n",
      "Epoch 2/30, Loss: 0.6728\n",
      "Epoch 3/30, Loss: 0.6671\n",
      "Epoch 4/30, Loss: 0.6641\n",
      "Epoch 5/30, Loss: 0.6618\n",
      "Epoch 6/30, Loss: 0.6611\n",
      "Epoch 7/30, Loss: 0.6611\n",
      "Epoch 8/30, Loss: 0.6611\n",
      "Epoch 9/30, Loss: 0.6610\n",
      "Epoch 10/30, Loss: 0.6611\n",
      "Epoch 11/30, Loss: 0.6610\n",
      "Epoch 12/30, Loss: 0.6611\n",
      "Epoch 13/30, Loss: 0.6611\n",
      "Epoch 14/30, Loss: 0.6611\n",
      "Epoch 15/30, Loss: 0.6610\n",
      "Epoch 16/30, Loss: 0.6611\n",
      "Epoch 17/30, Loss: 0.6610\n",
      "Epoch 18/30, Loss: 0.6610\n",
      "Epoch 19/30, Loss: 0.6610\n",
      "Epoch 20/30, Loss: 0.6610\n",
      "Epoch 21/30, Loss: 0.6610\n",
      "Epoch 22/30, Loss: 0.6611\n",
      "Epoch 23/30, Loss: 0.6611\n",
      "Epoch 24/30, Loss: 0.6611\n",
      "Epoch 25/30, Loss: 0.6610\n",
      "Epoch 26/30, Loss: 0.6610\n",
      "Epoch 27/30, Loss: 0.6610\n",
      "Epoch 28/30, Loss: 0.6610\n",
      "Epoch 29/30, Loss: 0.6610\n",
      "Epoch 30/30, Loss: 0.6610\n",
      "--- Training Complete ---\n",
      "\n",
      "--- Metrics for entanglement = linear ---\n",
      "entanglement: linear\n",
      "feature_map_reps: 1\n",
      "ansatz_reps: 1\n",
      "train_accuracy: 0.5835\n",
      "test_accuracy: 0.5695\n",
      "train_f1: 0.6415\n",
      "test_f1: 0.6192\n",
      "train_recall: 0.7454\n",
      "test_recall: 0.7000\n",
      "train_precision: 0.5631\n",
      "test_precision: 0.5551\n",
      "Train Confusion Matrix:\n",
      " [[2108 2892]\n",
      " [1273 3727]]\n",
      "Test Confusion Matrix:\n",
      " [[ 878 1122]\n",
      " [ 600 1400]]\n",
      "\n",
      "--- Training with entanglement = full ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/1950909632.py:57: DeprecationWarning: The class ``qiskit.primitives.sampler.Sampler`` is deprecated as of qiskit 1.2. It will be removed no earlier than 3 months after the release date. All implementations of the `BaseSamplerV1` interface have been deprecated in favor of their V2 counterparts. The V2 alternative for the `Sampler` class is `StatevectorSampler`.\n",
      "  sampler = Sampler()\n",
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/1950909632.py:58: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.\n",
      "  qnn = SamplerQNN(\n",
      "/Users/pawanpahune/New_Research_Deakin/venv/lib/python3.12/site-packages/qiskit_machine_learning/connectors/torch_connector.py:378: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self._weights.data = torch.tensor(initial_weights, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.6696\n",
      "Epoch 2/30, Loss: 0.6318\n",
      "Epoch 3/30, Loss: 0.6168\n",
      "Epoch 4/30, Loss: 0.6110\n",
      "Epoch 5/30, Loss: 0.6095\n",
      "Epoch 6/30, Loss: 0.6093\n",
      "Epoch 7/30, Loss: 0.6094\n",
      "Epoch 8/30, Loss: 0.6093\n",
      "Epoch 9/30, Loss: 0.6094\n",
      "Epoch 10/30, Loss: 0.6093\n",
      "Epoch 11/30, Loss: 0.6093\n",
      "Epoch 12/30, Loss: 0.6093\n",
      "Epoch 13/30, Loss: 0.6093\n",
      "Epoch 14/30, Loss: 0.6094\n",
      "Epoch 15/30, Loss: 0.6093\n",
      "Epoch 16/30, Loss: 0.6093\n",
      "Epoch 17/30, Loss: 0.6093\n",
      "Epoch 18/30, Loss: 0.6093\n",
      "Epoch 19/30, Loss: 0.6093\n",
      "Epoch 20/30, Loss: 0.6093\n",
      "Epoch 21/30, Loss: 0.6093\n",
      "Epoch 22/30, Loss: 0.6093\n",
      "Epoch 23/30, Loss: 0.6093\n",
      "Epoch 24/30, Loss: 0.6092\n",
      "Epoch 25/30, Loss: 0.6092\n",
      "Epoch 26/30, Loss: 0.6093\n",
      "Epoch 27/30, Loss: 0.6093\n",
      "Epoch 28/30, Loss: 0.6093\n",
      "Epoch 29/30, Loss: 0.6093\n",
      "Epoch 30/30, Loss: 0.6093\n",
      "--- Training Complete ---\n",
      "\n",
      "--- Metrics for entanglement = full ---\n",
      "entanglement: full\n",
      "feature_map_reps: 1\n",
      "ansatz_reps: 1\n",
      "train_accuracy: 0.8137\n",
      "test_accuracy: 0.7762\n",
      "train_f1: 0.8027\n",
      "test_f1: 0.7578\n",
      "train_recall: 0.7578\n",
      "test_recall: 0.7000\n",
      "train_precision: 0.8532\n",
      "test_precision: 0.8260\n",
      "Train Confusion Matrix:\n",
      " [[4348  652]\n",
      " [1211 3789]]\n",
      "Test Confusion Matrix:\n",
      " [[1705  295]\n",
      " [ 600 1400]]\n",
      "\n",
      "--- Training with entanglement = circular ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/1950909632.py:57: DeprecationWarning: The class ``qiskit.primitives.sampler.Sampler`` is deprecated as of qiskit 1.2. It will be removed no earlier than 3 months after the release date. All implementations of the `BaseSamplerV1` interface have been deprecated in favor of their V2 counterparts. The V2 alternative for the `Sampler` class is `StatevectorSampler`.\n",
      "  sampler = Sampler()\n",
      "/var/folders/ly/dps_wwpj7972w23hbq80tr9r0000gn/T/ipykernel_41057/1950909632.py:58: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.\n",
      "  qnn = SamplerQNN(\n",
      "/Users/pawanpahune/New_Research_Deakin/venv/lib/python3.12/site-packages/qiskit_machine_learning/connectors/torch_connector.py:378: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self._weights.data = torch.tensor(initial_weights, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.6750\n",
      "Epoch 2/30, Loss: 0.6483\n",
      "Epoch 3/30, Loss: 0.6328\n",
      "Epoch 4/30, Loss: 0.6243\n",
      "Epoch 5/30, Loss: 0.6198\n",
      "Epoch 6/30, Loss: 0.6177\n",
      "Epoch 7/30, Loss: 0.6169\n",
      "Epoch 8/30, Loss: 0.6166\n",
      "Epoch 9/30, Loss: 0.6165\n",
      "Epoch 10/30, Loss: 0.6164\n",
      "Epoch 11/30, Loss: 0.6165\n",
      "Epoch 12/30, Loss: 0.6165\n",
      "Epoch 13/30, Loss: 0.6165\n",
      "Epoch 14/30, Loss: 0.6164\n",
      "Epoch 15/30, Loss: 0.6164\n",
      "Epoch 16/30, Loss: 0.6165\n",
      "Epoch 17/30, Loss: 0.6165\n",
      "Epoch 18/30, Loss: 0.6164\n",
      "Epoch 19/30, Loss: 0.6164\n",
      "Epoch 20/30, Loss: 0.6165\n",
      "Epoch 21/30, Loss: 0.6165\n",
      "Epoch 22/30, Loss: 0.6165\n",
      "Epoch 23/30, Loss: 0.6165\n",
      "Epoch 24/30, Loss: 0.6165\n",
      "Epoch 25/30, Loss: 0.6165\n",
      "Epoch 26/30, Loss: 0.6164\n",
      "Epoch 27/30, Loss: 0.6165\n",
      "Epoch 28/30, Loss: 0.6165\n",
      "Epoch 29/30, Loss: 0.6165\n",
      "Epoch 30/30, Loss: 0.6164\n",
      "--- Training Complete ---\n",
      "\n",
      "--- Metrics for entanglement = circular ---\n",
      "entanglement: circular\n",
      "feature_map_reps: 1\n",
      "ansatz_reps: 1\n",
      "train_accuracy: 0.7300\n",
      "test_accuracy: 0.6845\n",
      "train_f1: 0.6964\n",
      "test_f1: 0.6357\n",
      "train_recall: 0.6192\n",
      "test_recall: 0.5505\n",
      "train_precision: 0.7955\n",
      "test_precision: 0.7520\n",
      "Train Confusion Matrix:\n",
      " [[4204  796]\n",
      " [1904 3096]]\n",
      "Test Confusion Matrix:\n",
      " [[1637  363]\n",
      " [ 899 1101]]\n"
     ]
    }
   ],
   "source": [
    "import sys, getopt, os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import PauliFeatureMap, RealAmplitudes\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.circuit.library import QNNCircuit\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "root_folder = 'QNNC_hybrid'\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ✅ CHANGE: Use 3 input features (matching CSVs)\n",
    "NUM_FEATURES = 3\n",
    "NUM_QUBITS = NUM_FEATURES\n",
    "NUM_TARGETS = 1\n",
    "\n",
    "# ✅ Keep only 1 value for reps (you wanted 3 training sessions only)\n",
    "FEATURE_MAP_REPS_LIST = [1]\n",
    "ANSATZ_REPS_LIST = [1]\n",
    "ENTANGLEMENT_LIST = ['linear', 'full', 'circular']\n",
    "\n",
    "# ✅ Tune for better speed\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "def get_qnn_torch_model(entangle, feature_map_reps, ansatz_reps):\n",
    "    input_params = ParameterVector(\"x\", NUM_FEATURES)\n",
    "    feature_map_template = PauliFeatureMap(feature_dimension=NUM_FEATURES, reps=feature_map_reps, entanglement=entangle)\n",
    "    feature_map = feature_map_template.assign_parameters(input_params)\n",
    "\n",
    "    ansatz_template = RealAmplitudes(NUM_QUBITS, reps=ansatz_reps, entanglement=entangle)\n",
    "    num_ansatz_params = ansatz_template.num_parameters\n",
    "    weight_params = ParameterVector(\"θ\", num_ansatz_params)\n",
    "    ansatz = ansatz_template.assign_parameters(weight_params)\n",
    "\n",
    "    qc = QNNCircuit(feature_map=feature_map, ansatz=ansatz_template)\n",
    "    parity = lambda x: \"{:b}\".format(x).count(\"1\") % 2\n",
    "    output_shape = 2\n",
    "\n",
    "    sampler = Sampler()\n",
    "    qnn = SamplerQNN(\n",
    "        circuit=qc,\n",
    "        interpret=parity,\n",
    "        output_shape=output_shape,\n",
    "        sampler=sampler,\n",
    "        sparse=False,\n",
    "        input_gradients=False,\n",
    "    )\n",
    "\n",
    "    initial_weights = 0.01 * (2 * np.random.rand(qnn.num_weights) - 1)\n",
    "    qnn_torch_model = TorchConnector(qnn, initial_weights=torch.tensor(initial_weights, dtype=torch.float32))\n",
    "    return qnn_torch_model.to(device)\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, qnn_model):\n",
    "        super().__init__()\n",
    "        self.qnn = qnn_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.qnn(x)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    date = '06_08_25_1'\n",
    "    os.makedirs(f'{root_folder}/result', exist_ok=True)\n",
    "    os.makedirs(f'{root_folder}/logs', exist_ok=True)\n",
    "\n",
    "    file_name = f'{root_folder}/result/FMR_1_AR_1_E_ALL_{date}.csv'\n",
    "\n",
    "    print(\"\\n--- Loading and Preprocessing Data ---\")\n",
    "\n",
    "    train_df = pd.read_csv(\"Training_Top3Features.csv\")\n",
    "    test_df = pd.read_csv(\"Testing_Top3Features.csv\")\n",
    "    X_train_raw = train_df.drop('went_on_backorder', axis=1).values\n",
    "    y_train = train_df['went_on_backorder'].values\n",
    "    X_test_raw = test_df.drop('went_on_backorder', axis=1).values\n",
    "    y_test = test_df['went_on_backorder'].values\n",
    "\n",
    "    print(f\"Train: {X_train_raw.shape}, Test: {X_test_raw.shape}\")\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(np.vstack([X_train_raw, X_test_raw]))\n",
    "    X_train = scaler.transform(X_train_raw)\n",
    "    X_test = scaler.transform(X_test_raw)\n",
    "\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_t = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    df = pd.DataFrame(columns=['entanglement', 'feature_map_reps', 'ansatz_reps',\n",
    "                               'train_accuracy', 'test_accuracy',\n",
    "                               'train_f1', 'test_f1',\n",
    "                               'train_recall', 'test_recall',\n",
    "                               'train_precision', 'test_precision'])\n",
    "\n",
    "    LOSS = nn.CrossEntropyLoss()\n",
    "\n",
    "    for entanglement in ENTANGLEMENT_LIST:\n",
    "        print(f\"\\n--- Training with entanglement = {entanglement} ---\")\n",
    "\n",
    "        model = HybridModel(get_qnn_torch_model(entangle=entanglement, feature_map_reps=1, ansatz_reps=1)).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = LOSS(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item() * batch_X.size(0)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {total_loss / len(train_loader.dataset):.4f}\")\n",
    "\n",
    "        print(\"--- Training Complete ---\")\n",
    "        model.eval()\n",
    "        def predict(loader):\n",
    "            all_preds, all_true = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in loader:\n",
    "                    outputs = model(batch_X.to(device)).cpu().numpy()\n",
    "                    all_preds.extend(outputs)\n",
    "                    all_true.extend(batch_y.cpu().numpy())\n",
    "            pred_classes = [1 if o[1] > o[0] else 0 for o in all_preds]\n",
    "            return np.array(all_true), np.array(pred_classes)\n",
    "\n",
    "        y_true_train, y_pred_train = predict(train_loader)\n",
    "        y_true_test, y_pred_test = predict(test_loader)\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            'entanglement': entanglement,\n",
    "            'feature_map_reps': 1,\n",
    "            'ansatz_reps': 1,\n",
    "            'train_accuracy': accuracy_score(y_true_train, y_pred_train),\n",
    "            'test_accuracy': accuracy_score(y_true_test, y_pred_test),\n",
    "            'train_f1': f1_score(y_true_train, y_pred_train),\n",
    "            'test_f1': f1_score(y_true_test, y_pred_test),\n",
    "            'train_recall': recall_score(y_true_train, y_pred_train),\n",
    "            'test_recall': recall_score(y_true_test, y_pred_test),\n",
    "            'train_precision': precision_score(y_true_train, y_pred_train),\n",
    "            'test_precision': precision_score(y_true_test, y_pred_test)\n",
    "        }\n",
    "        print(f\"\\n--- Metrics for entanglement = {entanglement} ---\")\n",
    "        for k, v in metrics.items():\n",
    "            if isinstance(v, float):\n",
    "                print(f\"{k}: {v:.4f}\")\n",
    "            else:\n",
    "                print(f\"{k}: {v}\")\n",
    "        print(\"Train Confusion Matrix:\\n\", confusion_matrix(y_true_train, y_pred_train))\n",
    "        print(\"Test Confusion Matrix:\\n\", confusion_matrix(y_true_test, y_pred_test))\n",
    "\n",
    "        df.loc[len(df)] = metrics\n",
    "        df.to_csv(file_name, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
